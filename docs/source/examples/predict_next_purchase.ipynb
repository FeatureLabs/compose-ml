{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Next Purchase\n",
    "\n",
    "In this example, we build a machine learning application that predicts whether customers will purchase a product within the next shopping period. This application is structured into three important steps:\n",
    "\n",
    "* Prediction Engineering\n",
    "* Feature Engineering\n",
    "* Machine Learning\n",
    "\n",
    "In the first step, we generate our own labels from the data by using [Compose](https://compose.alteryx.com/). In the second step, we generate the features for labels by using [Featuretools](https://docs.featuretools.com/). In the third step, we search for the best machine learning pipeline for the features and labels by using [EvalML](https://evalml.alteryx.com/). After working through these steps, you will learn how to build machine learning applications for real-world problems like predicting consumer spending. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo.predict_next_purchase import load_sample\n",
    "from evalml import AutoMLSearch\n",
    "from evalml.preprocessing import split_data\n",
    "import composeml as cp\n",
    "import featuretools as ft\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this historical data of online grocery orders provided by Instacart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_sample()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Engineering\n",
    "\n",
    "Note we have two parameters in the prediction problem:\n",
    "\n",
    "* The name of the product.\n",
    "* The length of the shopping period.\n",
    "\n",
    "We can change these parameters to create different prediction problems. For example, will a customer purchase an avocado within the next day or a banana within the next 5 days? These variations can be done by simply tweaking the parameters. This helps us explore different scenarios which is crucial for making better decisions.\n",
    "\n",
    "\n",
    "### Defining the Labeling Process\n",
    "\n",
    "In each shopping period, we will check whether a customer bought a product. Let’s define this as a labeling function with a parameter for the product name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bought_product(ds, product_name):\n",
    "    return ds.product_name.str.contains(product_name).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Prediction Problem\n",
    "\n",
    "We will represent the prediction problem using a label maker. This way, we can run searches on the online grocery orders to generate the training examples. This is done by setting the following parameters:\n",
    "\n",
    "* The `target_entity` as the customer, because we want to label orders for each individual customer.\n",
    "* The `labeling_function` as the function we defined previously.\n",
    "* The `time_index` as the order time, because shoppings periods are based on the order time.\n",
    "* The `window_size` as the length of a shopping period. We can tweak this parameter to create variations of the prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='user_id',\n",
    "    time_index='order_time',\n",
    "    labeling_function=bought_product,\n",
    "    window_size='3d',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Training Examples\n",
    "\n",
    "Now, we can run a search to find purchases of the product within the shopping periods of each customer. This is done using the following parameters:\n",
    "\n",
    "* The online grocery orders sorted by the order time.\n",
    "* The `num_examples_per_instance` to find the number of training examples per customer. We search for all existing examples.\n",
    "* The `product_name` as the product that we will check for purchases.\n",
    "* The `minimum_data`  \n",
    "\n",
    "The output from the search is a label times table with three columns:\n",
    "\n",
    "* The user ID associated to the online grocery orders.\n",
    "* The start time of the shopping period. This is also known as a cutoff time for building features. Only data that existed before the shopping period is valid to use for making predictions about the outcome.\n",
    "* Whether or not the product was purchased in the shopping period. This is calculated by our labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lm.search(\n",
    "    df.sort_values('order_time'),\n",
    "    num_examples_per_instance=-1,\n",
    "    product_name='Banana',\n",
    "    minimum_data='3d',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "lt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can become difficult to track the parameters that were used to create the labels. As a helpful reference, we can look at the label description to understand how the labels were created from the start. The description also shows us the label distribution which we can check for imbalanced labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a better look at the labels by plotting the distribution and the cumulative count across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = mpl.pyplot.figure(figsize=(5, 8))\n",
    "ax0 = fig.add_subplot(211)\n",
    "ax1 = mpl.pyplot.subplot(212)\n",
    "fig.tight_layout()\n",
    "\n",
    "lt.plot.distribution(ax=ax0)\n",
    "lt.plot.count_by_time(ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In the previous step, we generated the labels. The next step is to generate the features.\n",
    "\n",
    "### Representing the Data\n",
    "\n",
    "We will represent the online grocery orders using an entity set. This way, we can generate features based on the relational structure of the dataset. We currently have a single table of orders where one user can many orders. This one-to-many relationship can be represented in an entity set by normalizing an entity for the users. The same can be done for products, departments, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet('instacart')\n",
    "\n",
    "es.entity_from_dataframe(\n",
    "    dataframe=df.reset_index(),\n",
    "    entity_id='order_products',\n",
    "    time_index='order_time',\n",
    "    index='id',\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='order_products',\n",
    "    new_entity_id='orders',\n",
    "    index='order_id',\n",
    "    additional_variables=['user_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='orders',\n",
    "    new_entity_id='users',\n",
    "    index='user_id',\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='order_products',\n",
    "    new_entity_id='products',\n",
    "    index='product_id',\n",
    "    additional_variables=['aisle_id', 'department_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='products',\n",
    "    new_entity_id='aisles',\n",
    "    index='aisle_id',\n",
    "    additional_variables=['department_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='aisles',\n",
    "    new_entity_id='departments',\n",
    "    index='department_id',\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es[\"order_products\"][\"department\"].interesting_values = ['produce']\n",
    "es[\"order_products\"][\"product_name\"].interesting_values = ['Banana']\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Features\n",
    "\n",
    "Now, we can generate features by using a method called Deep Feature Synthesis (DFS). This will automatically build features by stacking and applying mathematical operations called primitives across relationships in an entity set. The more structured an entity set is, the better DFS can leverage the relationships to generate better features. Let’s run DFS using the following parameters:\n",
    "\n",
    "* The target entity as the user, because we want to generate features for each user. \n",
    "* The cutoff time as the labels that we created previously. \n",
    "\n",
    "There are two outputs from DFS: a feature matrix and feature definitions. The feature matrix is a table that contains the calculated feature values based on cutoff times from our labels. Feature definitions are features in a list that can be stored and reused later to calculate the same set of features on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, fd = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='users',\n",
    "    cutoff_time=lt,\n",
    "    cutoff_time_in_index=True,\n",
    "    include_cutoff_time=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Now, we can create a machine learning model. Let's extract the labels from the feature matrix and split the data into training and holdout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fm.pop('bought_product')\n",
    "splits = split_data(fm, y, test_size=0.2, random_state=0)\n",
    "X_train, X_holdout, y_train, y_holdout = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Next, we search for the optimal pipeline by trying out different models on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoMLSearch(problem_type='binary', objective='f1', random_state=0)\n",
    "automl.search(X_train, y_train, data_checks='disabled', show_iteration_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_pipeline.describe()\n",
    "automl.best_pipeline.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "\n",
    "Finally, we score the model performance by evaluating predictions on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = automl.best_pipeline.fit(X_train, y_train)\n",
    "score = best_pipeline.score(X_holdout, y_holdout, objectives=['f1'])\n",
    "dict(score)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
