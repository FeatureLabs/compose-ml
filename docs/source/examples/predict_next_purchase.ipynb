{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Next Purchase\n",
    "\n",
    "In this example, we build a machine learning application that predicts whether customers will purchase a product within the next shopping period. This application is structured into three important steps:\n",
    "\n",
    "* Prediction Engineering\n",
    "* Feature Engineering\n",
    "* Machine Learning\n",
    "\n",
    "In the first step, we generate new labels from the data by using [Compose](https://compose.alteryx.com/). In the second step, we generate features for the labels by using [Featuretools](https://docs.featuretools.com/). In the third step, we search for the best machine learning pipeline by using [EvalML](https://evalml.alteryx.com/). After working through these steps, you will learn how to build machine learning applications for real-world problems like predicting consumer spending. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo.predict_next_purchase import load_sample\n",
    "from matplotlib.pyplot import subplots\n",
    "import composeml as cp\n",
    "import featuretools as ft\n",
    "import evalml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this historical data of online grocery orders provided by Instacart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_sample()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Engineering\n",
    "\n",
    "> Will customers purchase a product within the next shopping period?\n",
    "\n",
    "In this prediction problem, we have two parameters:\n",
    "\n",
    "* The product that a customer can purchase.\n",
    "* The length of the shopping period.\n",
    "\n",
    "We can change these parameters to create different prediction problems. For example, will a customer purchase a banana within the next 5 days or an avocado within the next three weeks? These variations can be done by simply tweaking the parameters. This helps us explore different scenarios which is crucial for making better decisions.\n",
    "\n",
    "\n",
    "### Defining the Labeling Process\n",
    "\n",
    "Let's start by defining a labeling function that checks if a customer bought a given product. We will make the product a parameter of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bought_product(ds, product_name):\n",
    "    return ds.product_name.str.contains(product_name).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Prediction Problem\n",
    "\n",
    "Then, let's represent the prediction problem by creating a label maker with the following parameters:\n",
    "\n",
    "* The `target_entity` as the columns for the customer ID, since we want to process orders for each customer.\n",
    "* The `labeling_function` as the function we defined previously.\n",
    "* The `time_index` as the column for the order time, since shoppings periods are based on the order time.\n",
    "* The `window_size` as the length of a shopping period. We can easily change this parameter to create variations of the prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='user_id',\n",
    "    time_index='order_time',\n",
    "    labeling_function=bought_product,\n",
    "    window_size='3d',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Training Examples\n",
    "\n",
    "Now, let's run a search to get the training examples by using the following parameters:\n",
    "\n",
    "* The online grocery orders sorted by the order time.\n",
    "* The `num_examples_per_instance` to find the number of training examples per customer. In this case, we search for all existing examples.\n",
    "* The `product_name` as the product to check for purchases. This parameter get passed directly to the our labeling function.\n",
    "* The `minimum_data` as the minimum amount of data required to build features for the first label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lm.search(\n",
    "    df.sort_values('order_time'),\n",
    "    num_examples_per_instance=-1,\n",
    "    product_name='Banana',\n",
    "    minimum_data='3d',\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "lt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the search is a label times table with three columns:\n",
    "\n",
    "* The customer ID associated to the orders.\n",
    "* The start time of the shopping period. This is also known as a cutoff time for building features. Only data that existed beforehand is valid to use for predictions.\n",
    "* Whether or not the product was purchased in the shopping period. This is calculated by our labeling function.\n",
    "\n",
    "It can become difficult to track the parameters that were used to create the labels. As a helpful reference, we can look at the label description to understand how the labels were created from the start. The description also shows us the label distribution which we can check for imbalanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a better look at the labels by plotting the distribution and the cumulative count across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = subplots(nrows=2, ncols=1, figsize=(6, 8))\n",
    "lt.plot.distribution(ax=ax[0])\n",
    "lt.plot.count_by_time(ax=ax[1])\n",
    "fig.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In the previous step, we generated the labels. The next step is to generate the features.\n",
    "\n",
    "### Representing the Data\n",
    "\n",
    "We will represent the online grocery orders using an entity set. This way, we can generate features based on the relational structure of the dataset. We currently have a single table of orders where one customer can many orders. This one-to-many relationship can be represented in an entity set by normalizing an entity for the customers. The same can be done for departments, aisles, and products. Let's structure the entity set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet('instacart')\n",
    "\n",
    "es.entity_from_dataframe(\n",
    "    dataframe=df.reset_index(),\n",
    "    entity_id='order_products',\n",
    "    time_index='order_time',\n",
    "    index='id',\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='order_products',\n",
    "    new_entity_id='orders',\n",
    "    index='order_id',\n",
    "    additional_variables=['user_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='orders',\n",
    "    new_entity_id='customers',\n",
    "    index='user_id',\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='order_products',\n",
    "    new_entity_id='products',\n",
    "    index='product_id',\n",
    "    additional_variables=['aisle_id', 'department_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='products',\n",
    "    new_entity_id='aisles',\n",
    "    index='aisle_id',\n",
    "    additional_variables=['department_id'],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='aisles',\n",
    "    new_entity_id='departments',\n",
    "    index='department_id',\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es[\"order_products\"][\"department\"].interesting_values = ['produce']\n",
    "es[\"order_products\"][\"product_name\"].interesting_values = ['Banana']\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Features\n",
    "\n",
    "Now, we can generate features by using a method called Deep Feature Synthesis (DFS). This will automatically build features by stacking and applying mathematical operations called primitives across relationships in an entity set. The more structured an entity set is, the better DFS can leverage the relationships to generate better features. Letâ€™s run DFS using the following parameters:\n",
    "\n",
    "* The `entity_set` as the entity set we structured previously.\n",
    "* The `target_entity` as the customers, since we want to generate features for each customer. \n",
    "* The `cutoff_time` as the label times that we generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, fd = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='customers',\n",
    "    cutoff_time=lt,\n",
    "    cutoff_time_in_index=True,\n",
    "    include_cutoff_time=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two outputs from DFS: a feature matrix and feature definitions. The feature matrix is a table that contains the feature values based on the cutoff times from our labels. Feature definitions are features in a list that can be stored and reused later to calculate the same set of features on future data.\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "In the previous steps, we generated the labels and features. The final step is to build the machine learning pipeline.\n",
    "\n",
    "### Splitting the Data\n",
    "\n",
    "We will start by extracting the labels from the feature matrix and splitting the data into a training set and holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fm.pop('bought_product')\n",
    "splits = evalml.preprocessing.split_data(fm, y, test_size=0.2, random_state=0)\n",
    "X_train, X_holdout, y_train, y_holdout = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Model\n",
    "\n",
    "Then, we run a search on the training set for the best machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = evalml.AutoMLSearch(problem_type='binary', objective='f1', random_state=0)\n",
    "automl.search(X_train, y_train, data_checks='disabled', show_iteration_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the details and steps from the best pipeline found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_pipeline.describe()\n",
    "automl.best_pipeline.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's score the model performance by evaluating predictions on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = automl.best_pipeline.fit(X_train, y_train)\n",
    "score = best_pipeline.score(X_holdout, y_holdout, objectives=['f1'])\n",
    "dict(score)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
