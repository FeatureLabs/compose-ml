{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "# Predict Turbofan Degradation\n",
    "\n",
    "In this example, we build a machine learning application to predict turbofan engine degradation. This application is structured into three important steps:\n",
    "\n",
    "* Prediction Engineering\n",
    "* Feature Engineering\n",
    "* Machine Learning\n",
    "\n",
    "In the first step, we generate new labels from the data by using [Compose](https://compose.alteryx.com/). In the second step, we generate features for the labels by using [Featuretools](https://docs.featuretools.com/). In the third step, we search for the best machine learning pipeline by using [EvalML](https://evalml.alteryx.com/). \n",
    "After working through these steps, you will learn how to build machine learning applications for real-world problems like predictive maintenance. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from demo.turbofan_degredation import load_sample\n",
    "from matplotlib.pyplot import subplots\n",
    "import composeml as cp\n",
    "import featuretools as ft\n",
    "import evalml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will use a dataset provided by NASA simulating turbofan engine degradation. In this dataset, we have engines which are monitored over time. Each engine had operational settings and sensor measurements recorded for each cycle. The remaining useful life (RUL) is the amount of cycles an engine has left before it needs maintenance. What makes this dataset special is that the engines run all the way until failure, giving us precise RUL information for every engine at every point in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_sample()\n",
    "\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Engineering\n",
    "\n",
    "> Which range is the RUL of a turbofan engine in?\n",
    "\n",
    "In this prediction problem, we want to group the RUL into ranges. Then, predict which range the RUL is in. We can make variations of the ranges to create different prediction problems. For example, the ranges can be manually defined (0 - 150, 150 - 300, etc.) or based on the quartiles from historical observations. These variations can be done by simply binning the RUL. This helps us explore different scenarios which is crucial for making better decisions.\n",
    "\n",
    "### Defining the Labeling Function\n",
    "\n",
    "Let's start by defining the labeling function of an engine that calculates the RUL. Given that engines run all the way until failure, the RUL is just the remaining number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rul(ds):\n",
    "    return len(ds) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "### Representing the Prediction Problem\n",
    "\n",
    "Then, let's represent the prediction problem by creating a label maker with the following parameters:\n",
    "\n",
    "* The `target_entity` as the column for the engine ID, since we want to process records for each engine.\n",
    "* The `labeling_function` as the function we defined previously.\n",
    "* The `time_index` as the column for the event time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='engine_no',\n",
    "    labeling_function=rul,\n",
    "    time_index='time',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "### Finding the Training Examples\n",
    "\n",
    "Now, let's run a search to get the training examples by using the following parameters:\n",
    "\n",
    "* The records sorted by the event time, since the search will expect the records to be sorted chronologically, otherwise an error will be raised.\n",
    "* `num_examples_per_instance` as the number of training examples to find for each engine.\n",
    "* `minimum_data` as the amount of data that will be used to make features for the first training example.  \n",
    "* `gap` as the number of rows to skip between examples. This is done to cover different points in time of an engine.\n",
    "\n",
    "We can easily tweak these parameters and run more searches for training examples as the requirements of our model changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lm.search(\n",
    "    records.sort_values('time'),\n",
    "    num_examples_per_instance=20,\n",
    "    minimum_data=5,\n",
    "    gap=20,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "lt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The output from the search is a label times table with three columns:\n",
    "\n",
    "* The engine ID associated to the records. There can be many training examples generated from each engine.\n",
    "* The event time of the engine. This is also known as a cutoff time for building features. Only data that existed beforehand is valid to use for predictions.\n",
    "* The value of the RUL. This is calculated by our labeling function.\n",
    "\n",
    "At this point, we only have continuous values of the RUL. As a helpul reference, we can print out the search settings that were used to generate these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a better look at the values by plotting the distribution and the cumulative count across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = subplots(nrows=2, ncols=1, figsize=(6, 8))\n",
    "lt.plot.distribution(ax=ax[0])\n",
    "lt.plot.count_by_time(ax=ax[1])\n",
    "fig.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "With the continuous values, we can explore different ranges without running the search again. In this case, we will just use quartiles to bin the values into ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lt.bin(4, quantiles=True, precision=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "When we print out the settings again, we can now see that the description of the labels has been updated and reflects the latest changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the new label distribution and cumulative count across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(nrows=2, ncols=1, figsize=(6, 8))\n",
    "lt.plot.distribution(ax=ax[0])\n",
    "lt.plot.count_by_time(ax=ax[1])\n",
    "fig.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In the previous step, we generated the labels. The next step is to generate the features.\n",
    "\n",
    "### Representing the Data\n",
    "\n",
    "We will represent the data using an entity set. We currently have a single table of records where one engine can many records. This one-to-many relationship can be represented in an entity set by normalizing an entity for the engines. The same can be done for engine cycles. Let's start by structuring the entity set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet('observations')\n",
    "\n",
    "es.entity_from_dataframe(\n",
    "    dataframe=records.reset_index(),\n",
    "    entity_id='records',\n",
    "    index='id',\n",
    "    time_index='time',\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='records',\n",
    "    new_entity_id='engines',\n",
    "    index='engine_no',\n",
    ")\n",
    "\n",
    "es.normalize_entity(\n",
    "    base_entity_id='records',\n",
    "    new_entity_id='cycles',\n",
    "    index='time_in_cycles',\n",
    ")\n",
    "\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "### Calculating the Features\n",
    "\n",
    "Now, we can generate features by using a method called Deep Feature Synthesis (DFS). This will automatically build features by stacking and applying mathematical operations called primitives across relationships in an entity set. The more structured an entity set is, the better DFS can leverage the relationships to generate better features. Letâ€™s run DFS using the following parameters:\n",
    "\n",
    "* `entity_set` as the entity set we structured previously.\n",
    "* `target_entity` as the engines, since we want to generate features for each engine. \n",
    "* `cutoff_time` as the label times that we generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, fd = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='engines',\n",
    "    agg_primitives=['sum'],\n",
    "    trans_primitives=[],\n",
    "    cutoff_time=lt,\n",
    "    cutoff_time_in_index=True,\n",
    "    include_cutoff_time=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "\n",
    "There are two outputs from DFS: a feature matrix and feature definitions. The feature matrix is a table that contains the feature values based on the cutoff times from our labels. Feature definitions are features in a list that can be stored and reused later to calculate the same set of features on future data.\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "In the previous steps, we generated the labels and features. The final step is to build the machine learning pipeline.\n",
    "\n",
    "### Splitting the Data\n",
    "\n",
    "We will start by extracting the labels from the feature matrix and splitting the data into a training set and holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fm.pop('rul').cat.codes\n",
    "\n",
    "splits = evalml.preprocessing.split_data(\n",
    "    X=fm,\n",
    "    y=y,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "### Finding the Best Model\n",
    "\n",
    "Then, let's run a search on the training set for the best machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = evalml.AutoMLSearch(\n",
    "    problem_type='multiclass',\n",
    "    objective='f1_macro',\n",
    ")\n",
    "\n",
    "automl.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    data_checks='disabled',\n",
    "    show_iteration_plot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the search is complete, we can print out information about the best pipeline found, such as the parameters in each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_pipeline.describe()\n",
    "automl.best_pipeline.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now, let's score the model performance by evaluating predictions on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = automl.best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "score = best_pipeline.score(\n",
    "    X=X_holdout,\n",
    "    y=y_holdout,\n",
    "    objectives=['f1_macro'],\n",
    ")\n",
    "\n",
    "dict(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "From the pipeline, we can see which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = best_pipeline.feature_importance\n",
    "feature_importance = feature_importance.set_index('feature')['importance']\n",
    "top_k = feature_importance.abs().sort_values().tail(20).index\n",
    "feature_importance[top_k].plot.barh(figsize=(8, 8), fontsize=14, width=.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "At this point, we have completed the machine learning application. We can revisit each step to explore and fine-tune with different parameters until the model is ready for deployment. For more information on how to work with the features produced by Featuretools, take a look at [the Featuretools documentation](https://docs.featuretools.com/). For more information on how to work with the models produced by EvalML, take a look at [the EvalML documentation](https://evalml.alteryx.com/)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
